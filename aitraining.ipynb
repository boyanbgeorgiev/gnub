{"cells":[{"cell_type":"markdown","metadata":{},"source":["Biblioteki"]},{"cell_type":"code","execution_count":27,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline \n","import cv2\n","import os"]},{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"source":["Konstanti"]},{"cell_type":"code","execution_count":28,"metadata":{"_uuid":"d157dc1311928d1ada13ed2ef4a34c4ea5c0b538","trusted":true},"outputs":[],"source":["NUM_CLASSES = 14\n","CHANNELS = 3\n","IMAGE_RESIZE = 75\n","RESNET50_POOLING_AVERAGE = 'avg'\n","DENSE_LAYER_ACTIVATION = 'softmax'\n","OBJECTIVE_FUNCTION = 'categorical_crossentropy'\n","LOSS_METRICS = ['accuracy']\n","NUM_EPOCHS = 10\n","EARLY_STOP_PATIENCE = 3\n","STEPS_PER_EPOCH_TRAINING = 10\n","STEPS_PER_EPOCH_VALIDATION= 10\n","BATCH_SIZE_TRAINING = 100\n","BATCH_SIZE_VALIDATION = 100\n","BATCH_SIZE_TESTING = 1"]},{"cell_type":"markdown","metadata":{"_uuid":"b651539464d36c7443a601e914479f0d2153f435"},"source":["ResNet50"]},{"cell_type":"code","execution_count":29,"metadata":{"_uuid":"2fc47c3f887e6713b5a2be38c02b38a8ec80bf97","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.16.1\n"]}],"source":["import tensorflow as tf\n","print(tf.__version__)\n","import tensorflow as tf\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.models import Sequential\n","from tensorflow.python.keras import optimizers\n","resnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'"]},{"cell_type":"markdown","metadata":{},"source":["Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"670238611770f43a056332cc06efff44e20ad124","trusted":true},"outputs":[],"source":["\n","sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n","model.compile(optimizer = sgd, loss = OBJECTIVE_FUNCTION, metrics = LOSS_METRICS)"]},{"cell_type":"markdown","metadata":{"_uuid":"8d003a99fe4ff58b8905c7f6b5286d97a852cb7a"},"source":["### Prepare Keras Data Generators\n","\n","Keras *ImageDataGenerator(...)* generates batches of tensor image data with real-time data augmentation. The data will be looped over (in batches). It is useful with large dataset to source, pre-process (resize, color conversion, image augmentation, batch normalize) & supply resulting images in batches to downstream Keras modeling components, namely *fit_generator(...)* & *predict_generator(...)* -vs- *fit(...)* & *predict(...)* for small dataset.\n","\n","Kaggle competition rule expects Dog & Cat to be labeled as 1 & 0. Keras >> ImageDataGenerator >> flow_from_directory takes in 'classes' list for mapping it to LABEL indices otherwise treats sub-folders enumerated classes in alphabetical order, i.e., Cat is 0 & Dog is 1."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ac9ebd909fee81c8c8d9b9fccb6590944d8106eb","trusted":true},"outputs":[],"source":["from keras.applications.resnet50 import preprocess_input\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","image_size = IMAGE_RESIZE\n","\n","# preprocessing_function is applied on each image but only after re-sizing & augmentation (resize => augment => pre-process)\n","# Each of the keras.application.resnet* preprocess_input MOSTLY mean BATCH NORMALIZATION (applied on each batch) stabilize the inputs to nonlinear activation functions\n","# Batch Normalization helps in faster convergence\n","data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n","\n","# flow_From_directory generates batches of augmented data (where augmentation can be color conversion, etc)\n","# Both train & valid folders must have NUM_CLASSES sub-folders\n","train_generator = data_generator.flow_from_directory(\n","        '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/train',\n","        target_size=(image_size, image_size),\n","        batch_size=BATCH_SIZE_TRAINING,\n","        class_mode='categorical')\n","\n","validation_generator = data_generator.flow_from_directory(\n","        '../input/catsdogs-trainvalid-80pc-prepd/trainvalidfull4keras/trainvalidfull4keras/valid',\n","        target_size=(image_size, image_size),\n","        batch_size=BATCH_SIZE_VALIDATION,\n","        class_mode='categorical') "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5014f91b6aeae661f0bc5db5d546089a07ca5b9d","trusted":true},"outputs":[],"source":["# Max number of steps that these generator will have opportunity to process their source content\n","# len(train_generator) should be 'no. of available train images / BATCH_SIZE_TRAINING'\n","# len(valid_generator) should be 'no. of available train images / BATCH_SIZE_VALIDATION'\n","(BATCH_SIZE_TRAINING, len(train_generator), BATCH_SIZE_VALIDATION, len(validation_generator))"]},{"cell_type":"markdown","metadata":{"_uuid":"f56e7cf9eae4c86c72468e322e7f00859a2ce9cd"},"source":["### Train Our Model With Cats & Dogs Train (splitted) Data Set"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2dfad78129d725f42110cde0270c32d7373d6d1d","trusted":true},"outputs":[],"source":["# Early stopping & checkpointing the best model in ../working dir & restoring that as our model for prediction\n","from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","cb_early_stopper = EarlyStopping(monitor = 'val_loss', patience = EARLY_STOP_PATIENCE)\n","cb_checkpointer = ModelCheckpoint(filepath = '../working/best.hdf5', monitor = 'val_loss', save_best_only = True, mode = 'auto')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d108d3598210930bab431f43c6865488b01d467b","trusted":true},"outputs":[],"source":["# Grid Search is an ideal candidate for distributed machine learning\n","# Pseudo code for hyperparameters Grid Search\n","\n","'''\n","from sklearn.grid_search import ParameterGrid\n","param_grid = {'epochs': [5, 10, 15], 'steps_per_epoch' : [10, 20, 50]}\n","\n","grid = ParameterGrid(param_grid)\n","\n","# Accumulate history of all permutations (may be for viewing trend) and keep watching for lowest val_loss as final model\n","for params in grid:\n","    print(params)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"cf85fe3c0653aa56503b7da058ea8acf445eec6e","trusted":true},"outputs":[],"source":["fit_history = model.fit_generator(\n","        train_generator,\n","        steps_per_epoch=STEPS_PER_EPOCH_TRAINING,\n","        epochs = NUM_EPOCHS,\n","        validation_data=validation_generator,\n","        validation_steps=STEPS_PER_EPOCH_VALIDATION,\n","        callbacks=[cb_checkpointer, cb_early_stopper]\n",")\n","model.load_weights(\"../working/best.hdf5\")"]},{"cell_type":"markdown","metadata":{"_uuid":"1ef4dd95f6d12f9277255576645e8bfce5270b81"},"source":["### Training Metrics\n","\n","One of the default callbacks that is registered when training all deep learning models is the History callback. It records training metrics (training accuracy, training loss, validation loss & validation accuracy) for each epoch. Note that training accuracy & loss during epoch steps are somewhat incomplete information and they are not recorded in history.\n","\n","Observe that training uses early stopping, hence metrics is available for epochs run, not for NUM_EPOCHS."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"383d7a0aa87e1508a46fcebfcd5a80d7aafb840f","trusted":true},"outputs":[],"source":["print(fit_history.history.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"00e51568f7e6022c6738c0e1a41080bc7152c257","trusted":true},"outputs":[],"source":[" plt.figure(1, figsize = (15,8)) \n","    \n","plt.subplot(221)  \n","plt.plot(fit_history.history['acc'])  \n","plt.plot(fit_history.history['val_acc'])  \n","plt.title('model accuracy')  \n","plt.ylabel('accuracy')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'valid']) \n","    \n","plt.subplot(222)  \n","plt.plot(fit_history.history['loss'])  \n","plt.plot(fit_history.history['val_loss'])  \n","plt.title('model loss')  \n","plt.ylabel('loss')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'valid']) \n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3941e7c5d983e2ad6ae6d6da0a1415ea27e1db7e","trusted":true},"outputs":[],"source":["# NOTE that flow_from_directory treats each sub-folder as a class which works fine for training data\n","# Actually class_mode=None is a kind of workaround for test data which too must be kept in a subfolder\n","\n","# batch_size can be 1 or any factor of test dataset size to ensure that test dataset is samples just once, i.e., no data is left out\n","test_generator = data_generator.flow_from_directory(\n","    directory = '../input/test-files-prepd/test4keras/test4keras',\n","    target_size = (image_size, image_size),\n","    batch_size = BATCH_SIZE_TESTING,\n","    class_mode = None,\n","    shuffle = False,\n","    seed = 123\n",")\n","\n","# Try batch size of 1+ in test_generator & check batch_index & filenames in resulting batches\n","'''\n","for i in test_generator:\n","    #print(test_generator.batch_index, test_generator.batch_size)\n","    idx = (test_generator.batch_index - 1) * test_generator.batch_size\n","    print(test_generator.filenames[idx : idx + test_generator.batch_size])\n","'''"]},{"cell_type":"markdown","metadata":{"_uuid":"8c6e7f35552f87b522aa32397ffb71ea732ce80b","trusted":true},"source":["### Observe Prediction Time With Different Batch Size\n","\n","With GPU, 97s for full prediction with batch_size=100 -vs- 264s with 1. But note that to avoid ImageDataGenerator iterator repeatability, we need to use 1 as batch_size."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9911ff38ec4b82f5282f078aae63b2a510a9a6cc","trusted":true},"outputs":[],"source":["# Reset before each call to predict\n","test_generator.reset()\n","\n","pred = model.predict_generator(test_generator, steps = len(test_generator), verbose = 1)\n","\n","predicted_class_indices = np.argmax(pred, axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e54bc22bd1a2af7d704f9164b624b79b8cbd4f56","trusted":true},"outputs":[],"source":["TEST_DIR = '../input/test-files-prepd/test4keras/test4keras/'\n","f, ax = plt.subplots(5, 5, figsize = (15, 15))\n","\n","for i in range(0,25):\n","    imgBGR = cv2.imread(TEST_DIR + test_generator.filenames[i])\n","    imgRGB = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2RGB)\n","    \n","    # a if condition else b\n","    predicted_class = \"Dog\" if predicted_class_indices[i] else \"Cat\"\n","\n","    ax[i//5, i%5].imshow(imgRGB)\n","    ax[i//5, i%5].axis('off')\n","    ax[i//5, i%5].set_title(\"Predicted:{}\".format(predicted_class))    \n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"39bac850385ba7129c97c1c0d389b36e7f2f0dfe","trusted":true},"outputs":[],"source":["results_df = pd.DataFrame(\n","    {\n","        'id': pd.Series(test_generator.filenames), \n","        'label': pd.Series(predicted_class_indices)\n","    })\n","results_df['id'] = results_df.id.str.extract('(\\d+)')\n","results_df['id'] = pd.to_numeric(results_df['id'], errors = 'coerce')\n","results_df.sort_values(by='id', inplace = True)\n","\n","results_df.to_csv('submission.csv', index=False)\n","results_df.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"2e5600300a330725b29c74c813135144a22cee56"},"source":["### Keras Limitations\n","\n","* [10/02/2018] The *validation_split* is not supported in *fit_generator*, hence its expects ImageDataGenerator for pre-splitted train & valid.\n","* [10/02/2018] Model learning through *fit_generator* is not compatible for Sklearn *GridSearchCV* again *mostly* due to no support for *validation_split*."]},{"cell_type":"markdown","metadata":{"_uuid":"60a34703bd652a314983948d93ee2dfe6d760ec3"},"source":["### Followup Plan\n","\n","1. Scale and pad and avoid aspect ratio change of original image through Keras ImageDataGenerator pre-processing insfrastructure\n","2. Image augmentation\n","3. Pipeline\n","4. Distributed ML for Grid Search on Spark Cluster"]},{"cell_type":"markdown","metadata":{"_uuid":"422beefc8f408582e85292cec3fe8225228f2ab5"},"source":["### References\n","\n","1. [Transfer Learning by Dan B](https://www.kaggle.com/dansbecker/transfer-learning)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":38425,"sourceId":5441,"sourceType":"competition"},{"datasetId":6209,"sourceId":9900,"sourceType":"datasetVersion"},{"datasetId":57539,"sourceId":110819,"sourceType":"datasetVersion"},{"datasetId":57578,"sourceId":110930,"sourceType":"datasetVersion"}],"dockerImageVersionId":11105,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
